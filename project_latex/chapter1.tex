\chapter{مقدمه}


یادگیری ماشین و به‌ویژه شبکه‌های عصبی عمیق\LTRfootnote{Deep Nueral Networks (DNNs)} در سال‌های اخیر پیشرفت چشمگیری داشته‌اند و توانسته‌اند در حوزه‌های مختلفی مانند بینایی ماشین، پردازش زبان طبیعی و تشخیص گفتار، به عملکردی نزدیک به انسان دست یابند \cite{attention_is_all_u_need-46,Mask-R-CNN-20,Deep-Residual-Learning-21,
	bert-10}.
برای آن‌که این شبکه‌ها بتوانند در کاربردهای مختلف عملکرد خوبی داشته باشند، معمولاً نیاز به انجام پیش‌آموزش در مقیاس بزرگ دارند \cite{attention_is_all_u_need-46,An-image-is-worth-11}.
انجام پیش‌آموزش، امکان استفاده‌ی دوباره‌‌ از این شبکه‌ها در وظایف\LTRfootnote{Tasks}
بعدی را فراهم می‌آورد \cite{Multi-Task-Feature-Learning-4}.
در روش‌های متداول برای آموزش شبکه‌های عصبی، فرض می‌شود که تمام داده‌های آموزشی مورد نیاز، در ابتدای کار در دسترس هستند و از این‌رو این روش‌ها از تمام داده‌ها به‌صورت هم‌زمان برای آموزش استفاده می‌کنند \cite{l2p}. با این حال در بسیاری از کاربردهای دنیای واقعی، این فرض که دسترسی به تمام داده‌های آموزشی، به‌صورت همزمان، وجود داشته باشد برقرار نیست. در واقع بسیاری از سناریوها وجود دارند که در آن داده‌ها به مرور زمان و در قالب وظایف متفاوت در دسترس قرار می‌گیرد. اخیراً دسته‌ای از روش‌های یادگیری ارائه شده اند که برای مواجهه با این سناریوها بکار گرفته می‌شوند. این روش‌های یادگیری با عنوان یادگیری پیوسته\LTRfootnote{Continual learning}
 شناخته ‌می‌شوند \cite{1,2}. در این نوع یادگیری، مدل باید بتواند دانش جدید را بیاموزد، بدون آن‌که دانش پیشین خود را از دست بدهد. یکی از چالش‌های اساسی در این زمینه، فراموشی فاجعه‌بار\LTRfootnote{Catastrophic forgetting}~\cite{Catastrophic_forgetting} است که باعث می‌شود مدل پس از یادگیری وظایف جدید، عملکرد خود را روی وظایف قبلی به‌طور چشمگیری از دست بدهد. براساس \cite{1}، رویکردهای یادگیری پیوسته به‌طور کلی به چهار دسته تقسیم می‌شوند: رویکردهای مبتنی بر تنظیم که با منظم‌سازی وزن‌ها تعادل بین یادگیری وظایف جدید و حفظ وظایف قبلی را برقرار می‌کنند؛ رویکردهای مبتنی بر بازپخش که با ذخیره یا بازتولید داده‌ها و ویژگی‌های وظایف گذشته و ترکیب آن‌ها با داده‌های جدید از فراموشی جلوگیری می‌کنند؛ رویکردهای مبتنی بر بهینه‌سازی که با اصلاح فرآیند به‌روزرسانی پارامترها مانع تغییرات ناسازگار با وظایف قبلی می‌شوند و رویکردهای مبتنی بر معماری که با طراحی یا اختصاص بخش‌های خاص مدل برای هر وظیفه، تداخل وظایف را کاهش داده و حفظ دانش پیشین را تسهیل می‌کنند. این روش‌ها با وجود آن‌که توانسته‌اند به نتایج قابل قبولی در حوزه‌ی یادگیری پیوسته دست یابند، با چالش‌هایی نیز مواجه هستند. به‌عنوان مثال، در روش مبتنی بر بازپخش، حافظه‌ی مصرفی با زیادشدن تعداد وظایف، افزایش می‌یابد. این محدودیت حافظه، سبب می‌شود که تعداد وظایف نتواند از حد خاصی بیشتر شود. در روش مبتنی بر معماری نیز محدویت حافظه می‌تواند به‌طور مشابه رخ دهد. به این ترتیب که با اضافه شدن وظایف جدید، بایستی لایه‌های جدیدی به شبکه اضافه شود که این امر نیازمند تخصیص حافظه می‌باشد. در روش‌های مبتنی بر تنظیم نیز، محدودیت ظرفیت شبکه سبب محدود شدن تعداد وظایفی می‌شود که شبکه می‌تواند آن را یاد بگیرد \cite{1,2}. 
 
 
 
  اخیراً، معرفی مدل‌های مبتنی بر سازوکار توجه\LTRfootnote{Attention mechanism}~\cite{bert-10,attention_is_all_u_need-46}،
 مدل‌های زبانی بزرگ\LTRfootnote{Large Language Models (LLMs)}    \cite{llm-example,gpt4} و مدل‌های بینایی-زبان\LTRfootnote{Vision–Language Models (VLMs)} \cite{clip,17}، زمینه‌ی جدیدی را برای ارائه‌ی روش‌های یادگیری پیوسته فراهم آورده‌اند\cite{llm_continual}. مدل‌های زبانی بزرگ و مدل‌های بینایی-زبان، نوعی شبکه‌ی عصبی مبتنی بر سازوکار توجه هستند که با آموزش بر روی حجم بسیار زیادی از داده‌، توانایی تولید، درک و تحلیل زبان طبیعی\LTRfootnote{Natural language processing} و تصویری را به‌دست آورده‌اند. در پی این موفقیت‌ها، تحقیقات متعددی به بهره‌گیری از آن‌ها در حوزه‌ی یادگیری پیوسته پرداخته‌اند \cite{llm_continual,l2p,clip-poolprompt,CoOp,ddas-2024,distillation}. با وجود آن‌که استفاده از مدل‌های زبانی بزرگ و مدل‌های بینایی-زبان، سبب افزایش صحت در وظایف یادگیری شده، چالش‌های روش‌های پیشین یادگیری پیوسته را برطرف نکرده است. در واقع به علت حجیم بودن این مدل‌ها، مشکلاتی مانند محدودیت حافظه پررنگ‌تر نیز شده است. از این رو برای مواجهه با این محدودیت، تمرکز محققین به سمت روش‌های مبتنی بر معماری رفته است \cite{llm_continual}. ایده‌ی اصلی این روش‌ها آن است که با ایجاد تغییر در ساختار مدل، از فراموشی فاجعه‌بار جلوگیری کنند. این تغییر در ساختار مدل‌های زبانی بزرگ شامل تنظیم پرامپت\LTRfootnote{Prompt tuning}، تنظیم پیشوند\LTRfootnote{Prefix tuning}، سازگاری رتبه پایین\LTRfootnote{low-rank adaptation (LoRA)}، وفق‌دهنده\LTRfootnote{Adapter} و مخلوط خبره‌ها \LTRfootnote{mixture of experts (MoE)} می‌شود \cite{llm_continual}. در میان این روش‌ها، مدل
 \lr{L2P}~\cite{l2p}
 با بهره‌گیری از تنظیم پرامپت، توانسته است به نتایج برجسته‌ای نسبت به سایر روش‌های مشابه دست ‌یابد. این مدل، روشی برای یادگیری پیوسته معرفی می‌کند که به جای تغییر پارامترهای اصلی مدل \lr{CLIP}، از گروهی پرامپت قابل‌آموزش (استخر پرامپت\LTRfootnote{Prompt pool}) استفاده می‌کند.
مدل 
\lr{L2P}
در هر گام آموزشی خود، پرامپت‌های مشابه داده‌های جدید را انتخاب و به‌روزرسانی می‌کند تا مدل بتواند دانش تازه را بیاموزد؛ بدون آنکه دانش قبلی خود را فراموش کند. این رویکرد، تعادلی مؤثر بین حفظ دانش گذشته و یادگیری وظایف جدید ایجاد کرده است. 
مدل 
\lr{L2P}
با وجود آن‌که توانسته است عملکرد خوبی داشته باشد و با چالش محدودیت حافظه در یادگیری پیوسته تا حد خوبی مقابله کند، تنها برای داده‌های تصویری قابل استفاده است. 
در زمینه‌ی استفاده از مدل‌های بینایی-زبان برای داده‌های ویدیویی نیز مطالعات متعددی انجام شده است که در این میان، می‌توان به مدل
\lr{Open-VCLIP}~\cite{open-vclip} 
به‌عنوان یکی از برترین روش‌ها از نظر عملکرد و بهینه‌بودن حجم مدل در زمینه‌ی درک داده‌ی ویدیویی اشاره کرد. مدل \lr{Open-VCLIP} با توسعه‌ی معماری \lr{CLIP}، امکان تحلیل ویدیو را فراهم می‌کند و با بهره‌گیری از تکنیک‌هایی، ضمن یادگیری دانش جدید حاصل از داده‌های ویدیویی، از فراموشی دانش مدل \lr{CLIP} جلوگیری کرده و تعمیم‌پذیری مدل را بهبود می‌بخشد. 
 هدف اصلی این تحقیق، ارائه‌ی رویکردی کارآمد برای یادگیری پیوسته در داده‌های ویدیویی است که بتواند با حداقل منابع محاسباتی، هم دانش پیشین را حفظ کند و هم دانش جدید را بیاموزد. برای تحقق این امر، روش پیشنهادی \lr{ProActionCLIP} با ترکیب قابلیت‌های \lr{Open-VCLIP} در استخراج ویژگی‌های داده‌ی ویدیویی و سازوکار پرامپت‌های یادگیرنده در \lr{L2P}، توسعه یافته است. این ترکیب، بدون نیاز به تغییر مستقیم پارامترهای مدل \lr{Open-VCLIP}، امکان تطبیق مدل با وظایف پیوسته را فراهم کرده، مشکل فراموشی فاجعه‌بار را کاهش می‌دهد، نیاز حافظه را به حداقل می‌رساند.
  نتایج آزمایش‌های تجربی بر روی مجموعه‌داده‌های 
 \lr{UCF101}\cite{ucf101} و
 \lr{HMDB1}\cite{hmdb51}
 نشان می‌دهد که مدل \lr{ProActionCLIP} توانسته است میانگین صحت و میزان فراموشی را نسبت به سایر روش‌های مشابه، بهبود ببخشد. 

ساختار نگارش این تحقیق به این صورت است که در فصل \ref{chap2:methods}، کارهای پیشین در حوزه یادگیری پیوسته مرور و رویکردهای موجود دسته‌بندی می‌شوند؛ در فصل \ref{chap3:proposed_method} روش پیشنهادی معرفی و جزئیات فنی آن بررسی می‌شود؛ در فصل \ref{chap4:results} نتایج آزمایش‌های تجربی ارائه و عملکرد روش پیشنهادی با روش‌های موجود مقایسه می‌شود و در فصل \ref{chap5:concolusion} جمع‌بندی نتایج انجام و پیشنهادهایی برای کارهای آینده مطرح خواهد شد.
























